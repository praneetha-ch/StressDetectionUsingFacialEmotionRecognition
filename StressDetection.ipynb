{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praneetha-ch/StressDetectionUsingFacialEmotionRecognition/blob/main/StressDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Modules"
      ],
      "metadata": {
        "id": "EPZrOzFadTW6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ru3EIUvm7J0m"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True, timeout_ms=360000)\n",
        "\n",
        "!cp '/content/drive/MyDrive/CK+.zip' \"CK+48\"\n",
        "!unzip \"/content/CK+48\" -d /content/CK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrjHehaY6_2z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50, VGG16\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from skimage.transform import resize\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIKYrLtZ6_21"
      },
      "source": [
        "#Loading the Dataset and Counting Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxHF7yZ16_21"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the main path to the dataset\n",
        "dataset_path = '/content/CK/ck/CK+48/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMVfNb-C-v2t"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "stress_emotions = [\"anger\", \"contempt\", \"disgust\", \"fear\", \"sadness\"]\n",
        "non_stress_emotions = [\"happy\", \"surprise\"]\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for folder in os.listdir(dataset_path):\n",
        "  images_path = os.path.join(dataset_path, folder)\n",
        "  images = os.listdir(images_path)\n",
        "  for image in images:\n",
        "    img_arr = cv2.imread(images_path+\"/\"+image)\n",
        "    X.append(img_arr)\n",
        "    if folder in stress_emotions:\n",
        "      y.append(1)\n",
        "    else:\n",
        "      y.append(0)\n",
        "X = np.array(X, dtype=int)\n",
        "y = np.array(y, dtype=int)\n",
        "\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=2)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ResNet50 Model"
      ],
      "metadata": {
        "id": "euQYFZrueFsa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfAR5nXj6_24"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load pre-trained ResNet50 model\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
        "\n",
        "# Freeze the base model layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create a new model on top of the ResNet50 base model\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "model.build(input_shape=(None, 48, 48, 3))\n",
        "print(model.summary())\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"best_model.keras\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBV4gcqM6_24"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "# Record the start time before training begins\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "# Your model training code here\n",
        "# model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "# Calculate the total time taken for training\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=100,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    callbacks=[checkpoint, reduce_lr, early_stopping])\n",
        "end_time = datetime.datetime.now()\n",
        "\n",
        "time_taken = end_time - start_time\n",
        "print(f\"Time taken for training: {time_taken}\")\n",
        "\n",
        "# Save the model weights\n",
        "model.save_weights('/content/best_model.weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1TQgP8rSqfs"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def plot_accuracies(history):\n",
        "    plt.plot(history.history['accuracy'], label=\"train\")\n",
        "    plt.plot(history.history['val_accuracy'], label=\"val\")\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    #plt.xticks(history.epoch, rotation=90)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "def plot_losses(history):\n",
        "  plt.plot(history.history['loss'], label=\"train\")\n",
        "  plt.plot(history.history['val_loss'], label=\"val\")\n",
        "  plt.title('Model Loss')\n",
        "  plt.ylabel('Loss')\n",
        "  #plt.xticks(history.epoch, rotation=90)\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.legend(loc='upper right')\n",
        "  plt.show()\n",
        "\n",
        "def plot_confusion_matrix(model, X_test, y_test):\n",
        "  y_pred = model.predict(X_test)\n",
        "  y_pred = np.argmax(y_pred, axis=1)\n",
        "  y_test = np.argmax(y_test, axis=1)\n",
        "  confusion_mat = confusion_matrix(y_test, y_pred)\n",
        "  sns.heatmap(confusion_mat, annot=confusion_mat, xticklabels=[\"non stress\", \"stress\"],\n",
        "              yticklabels=[\"non stress\", \"stress\"], fmt=\".0f\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxIY4xADSZI0"
      },
      "outputs": [],
      "source": [
        "plot_accuracies(history)\n",
        "plot_losses(history)\n",
        "plot_confusion_matrix(model, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#VGG16 Model"
      ],
      "metadata": {
        "id": "XLaGOxveeRvF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIN0umRu6_27"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load pre-trained VGG16 model\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
        "\n",
        "# Freeze the base model layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create a new model on top of the VGG16 base model\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.build(input_shape=(None, 48, 48, 3))\n",
        "print(model.summary())\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"best_model_VGG.keras\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTrBS5RjAbcx"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "import datetime\n",
        "start_time=datetime.datetime.now()\n",
        "history_vgg = model.fit(X_train, y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=50,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    callbacks=[checkpoint, reduce_lr, early_stopping])\n",
        "end_time=datetime.datetime.now()\n",
        "time_taken=end_time-start_time\n",
        "print(f\"Time taken for training: {time_taken}\")\n",
        "\n",
        "# Save the model weights\n",
        "model.save_weights('/content/best_model_vgg.weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Llg9uG1kKmNf"
      },
      "outputs": [],
      "source": [
        "plot_accuracies(history_vgg)\n",
        "plot_losses(history_vgg)\n",
        "plot_confusion_matrix(model, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Proposed Model"
      ],
      "metadata": {
        "id": "BehYbOVieWxl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVMmI-gTEhK_"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Activation, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "#1st CNN layer\n",
        "model.add(Conv2D(64,(3,3),padding = 'same',input_shape = (48,48,3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "#2nd CNN layer\n",
        "model.add(Conv2D(128,(5,5),padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "model.add(Dropout (0.25))\n",
        "\n",
        "#3rd CNN layer\n",
        "model.add(Conv2D(512,(3,3),padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "model.add(Dropout (0.25))\n",
        "\n",
        "#4th CNN layer\n",
        "model.add(Conv2D(512,(3,3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "#Fully connected 1st layer\n",
        "model.add(Dense(256))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "# Fully connected layer 2nd layer\n",
        "model.add(Dense(512))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "# Fully connected layer 3rd layer\n",
        "model.add(Dense(512))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.22))\n",
        "\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"best_model_new_arch.keras\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vohZTQtnGNzk"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "# Record the start time before training begins\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "# Your model training code here\n",
        "# model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "history_new_arch = model.fit(X_train, y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=100,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    callbacks=[checkpoint, reduce_lr, early_stopping])\n",
        "# Record the end time after training is done\n",
        "end_time = datetime.datetime.now()\n",
        "\n",
        "# Calculate the total time taken for training\n",
        "time_taken = end_time - start_time\n",
        "print(f\"Time taken for training: {time_taken}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8peanj1bG32"
      },
      "outputs": [],
      "source": [
        "plot_accuracies(history_new_arch)\n",
        "plot_losses(history_new_arch)\n",
        "plot_confusion_matrix(model, X_test, y_test)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 65125,
          "sourceId": 128470,
          "sourceType": "datasetVersion"
        },
        {
          "isSourceIdPinned": true,
          "modelId": 122377,
          "modelInstanceId": 98199,
          "sourceId": 116824,
          "sourceType": "modelInstanceVersion"
        }
      ],
      "dockerImageVersionId": 30698,
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}